(window.webpackJsonp=window.webpackJsonp||[]).push([[45],{172:function(t,e,o){"use strict";e.a={name:"VueRemarkRoot",render:function(t){return t("div",null,this.$slots.default)}}},395:function(t,e,o){"use strict";o.r(e);var n=o(4),a=o(172),i=o(0);function r(t){return(r="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&"function"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?"symbol":typeof t})(t)}i.default.config.optionMergeStrategies;var s={VueRemarkRoot:a.a},l=function(t){var e=t.options.components=t.options.components||{},o=t.options.computed=t.options.computed||{};Object.keys(s).forEach((function(t){"object"===r(s[t])&&"function"==typeof s[t].render?e[t]=s[t]:o[t]=function(){return s[t]}}))},p=i.default.config.optionMergeStrategies,d="__vueRemarkFrontMatter",u={excerpt:"Automated big data pipelines for healthcare data using Spark",title:"Data Engineer at HealthVerity",position:"Data Engineer",company:"HealthVerity",date:"2020-08-04T00:00:00.000Z",endDate:null,img:"healthverity.jpg",hours:40,tags:["python","pyspark","aws-emr","aws","airflow","spark","sql","spark-sql","etl","bash","make","docker","docker-compose","jenkins","zeppelin","big-data","hive","hadoop","healthcare"]};var c=function(t){t.options[d]&&(t.options[d]=u),i.default.util.defineReactive(t.options,d,u),t.options.computed=p.computed({$frontmatter:function(){return t.options[d]}},t.options.computed)},f=Object(n.a)({},(function(){var t=this,e=t.$createElement,o=t._self._c||e;return o("VueRemarkRoot",[o("ul",[o("li",[t._v("Automated big data normalization pipelines processing 5-10 terabytes of healthcare data per day. Most normalization logic was written in Spark SQL and utilized custom UDFs.")]),o("li",[t._v("Built utility libraries and tooling around PySpark and AWS EMR to accelerate normalization debugging and development for my team and others.")]),o("li",[t._v("Managed data lake and data warehouse utilizing Hadoop and Hive")]),o("li",[t._v("Improved internal ETL pipeline library built on top of Airflow to handle a wider range of data cases.")]),o("li",[t._v("Troubleshooted various data pipeline issues from data sizing problems to inconsistent data delivery, schemas, and formats. Improved checks, logging, and notification systems around ingestion and pre-normalization data validation.")]),o("li",[t._v("Increased tooling around AWS EMR cluster management for one-off normalization jobs.")]),o("li",[t._v("Improved data normalization testing through schema-based representative data generation, Docker Compose orchestration, and Zeppelin integration.")]),o("li",[t._v("Migrated legacy normalization jobs from Redshift. Optimized Spark configurations.")]),o("li",[t._v("Led efforts to develop and launch internal wiki for knowledge sharing within the organization")]),o("li",[t._v("Assisted security team in performing pentesting of systems. Assisted in various security efforts.")])])])}),[],!1,null,null,null);"function"==typeof l&&l(f),"function"==typeof c&&c(f);e.default=f.exports}}]);