(window.webpackJsonp=window.webpackJsonp||[]).push([[45],{UQSp:function(t,e,o){"use strict";e.a={name:"VueRemarkRoot",render(t){return t("div",null,this.$slots.default)}}},hp96:function(t,e,o){"use strict";o.r(e);var n=o("KHd+"),i=o("UQSp"),a=o("Kw5r");function r(t){return(r="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&"function"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?"symbol":typeof t})(t)}a.default.config.optionMergeStrategies;var s={VueRemarkRoot:i.a},p=function(t){var e=t.options.components=t.options.components||{},o=t.options.computed=t.options.computed||{};Object.keys(s).forEach((function(t){"object"===r(s[t])&&"function"==typeof s[t].render||"function"==typeof s[t]&&"function"==typeof s[t].options.render?e[t]=s[t]:o[t]=function(){return s[t]}}))},l=a.default.config.optionMergeStrategies,u="__vueRemarkFrontMatter",c={excerpt:"Automated big data pipelines for healthcare data using Spark",title:"Data Engineer at HealthVerity",position:"Data Engineer",company:"HealthVerity",date:"2020-08-04T00:00:00.000Z",endDate:"2021-10-01T00:00:00.000Z",img:"healthverity.jpg",hours:40,tags:["python","pyspark","aws-emr","aws","airflow","spark","sql","spark-sql","etl","bash","make","docker","docker-compose","jenkins","zeppelin","big-data","hive","hadoop","healthcare"]};var d=function(t){t.options[u]&&(t.options[u]=c),a.default.util.defineReactive(t.options,u,c),t.options.computed=l.computed({$frontmatter:function(){return t.options[u]}},t.options.computed)},f=Object(n.a)({},(function(){var t=this._self._c;return t("VueRemarkRoot",[t("ul",[t("li",[this._v("Automated big data normalization pipelines processing 5-10 terabytes of healthcare data per day.")]),t("li",[this._v("Built utility libraries and tooling around PySpark and AWS EMR to accelerate normalization debugging and development.")]),t("li",[this._v("Improved internal data pipeline library built on top of Airflow to handle a wider range of data cases.")]),t("li",[this._v("Improved data normalization testing through schema-based representative data generation.")]),t("li",[this._v("Migrated legacy normalization jobs from Redshift. Optimized Spark configurations.")]),t("li",[this._v("Led efforts to develop and launch internal wiki for knowledge sharing within the organization")]),t("li",[this._v("Assisted security team in performing pentesting of systems. Assisted in various security efforts.")])])])}),[],!1,null,null,null);"function"==typeof p&&p(f),"function"==typeof d&&d(f);e.default=f.exports}}]);